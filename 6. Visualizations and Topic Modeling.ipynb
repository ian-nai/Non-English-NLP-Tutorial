{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic modeling uses statistical models to discover the topics present in a text or group of texts.\n",
    "# We're going to use the LDA (Latent Dirichlet Allocation) approach for topic modeling in this module.\n",
    "# But first, we're going to explore some visualization techniques we can apply to our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we'll load our CSV, look at the data and see what information we can glean from it,\n",
    "# then create a bar graph of the parts of speech in the CSV we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Open our CSV and read the data\n",
    "document = pandas.read_csv('text_data.csv')\n",
    "\n",
    "# Print the first 5 lines of our document to check that it loaded properly\n",
    "print(document.head())\n",
    "\n",
    "# Detect our Parts of Speech ('pos') column in the CSV\n",
    "pos_group = document.groupby('pos')\n",
    "\n",
    "# Print a summary statistic of all parts of speech grouped together\n",
    "print(pos_group.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've learned how to manipulate the data a bit, we'll create a wordcloud with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the different words in our CSV into one long string\n",
    "long_string = ','.join(list(document.text.values))\n",
    "\n",
    "# Set up our wordcloud parameters\n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                min_font_size = 10).generate(long_string) \n",
    "  \n",
    "# create the wordcloud as an image                   \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we'll graph the 10 most common words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Set the text of our document as the word list\n",
    "word_list = list(document.text.values)\n",
    "\n",
    "# Get the 10 most common words from the list\n",
    "counts = dict(Counter(word_list).most_common(10))\n",
    "\n",
    "# Set up our labels\n",
    "labels, values = zip(*counts.items())\n",
    "\n",
    "# Sort our values in descending order\n",
    "indSort = np.argsort(values)[::-1]\n",
    "\n",
    "# Rearrange our data\n",
    "labels = np.array(labels)[indSort]\n",
    "values = np.array(values)[indSort]\n",
    "\n",
    "indexes = np.arange(len(labels))\n",
    "\n",
    "bar_width = 0.35\n",
    "\n",
    "plt.bar(indexes, values)\n",
    "\n",
    "# Add our labels\n",
    "plt.xticks(indexes + bar_width, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll use LDA model in SciKit Learn to conduct our topic modeling\n",
    "# You can read more about topic modeling in Python at this link: \n",
    "# https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730\n",
    "# The article above also provides code on how to use SciKit Learn to perform\n",
    "# NMF topic modeling on a text, which might be used and compared to our LDA results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Load our document\n",
    "document = pandas.read_csv('text_data.csv')\n",
    "\n",
    "# Set our tokens as the 'lemma' column in our CSV\n",
    "tokens = document.lemma.values\n",
    "\n",
    "# SciKit Learn setup\n",
    "documents = tokens\n",
    "\n",
    "# Our number of topics and features\n",
    "# Let's say we want 10 topics from the text\n",
    "no_topics = 20\n",
    "\n",
    "# The number of top words we'll include in our corpus \n",
    "no_features = 20000\n",
    "\n",
    "# Setting up our LDA settings...\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features)\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "# Running the LDA model\n",
    "lda = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "no_top_words = 10\n",
    "print(lda, tf_feature_names, no_top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
