{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This module contains code for performing named entity recognition and part-of-speech tagging. For this tutorial, we'll use [spaCy](https://github.com/ian-nai/Non-English-NLP-Tutorial/blob/master/Documentation%20Resources.md#spacy) to complete these tasks.\n",
    "\n",
    "##### Named entity recognition (NER) is the process of locating and classifying named entities in text into pre-defined categories (persons, places, and so on). Part-of-speech tagging tags the parts of speech computationally identified within a text.\n",
    "\n",
    "\n",
    "##### First, let's try performing NER with spaCy. Please note that the NER results are often inaccurate because we didn't train our model. For more information about training spaCy models, consult spaCy documentation at this link: https://spacy.io/usage/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import csv\n",
    "\n",
    "with open('cleaned_text.txt', 'r') as file:\n",
    "    text_data = file.read()\n",
    "    \n",
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "doc = nlp(text_data)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "    \n",
    "# Output our tagged data into a CSV\n",
    "with open('pos_tags.csv', 'w') as csvfile:\n",
    "    fieldnames = ['text', 'start_char', 'end_char', 'label']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        writer.writerow({'text': ent.text, 'start_char': ent.start_char, 'end_char': ent.end_char, 'label': ent.label})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can perform part-of-speech tagging using the spacy-lefff library. From the library's [GitHub repository](https://github.com/sammous/spacy-lefff):\n",
    "##### \"This package allows to bring Lefff lemmatization and part-of-speech tagging to a spaCy custom pipeline. When POS tagging and Lemmatization are combined inside a pipeline, it improves your text preprocessing for French compared to the built-in spaCy French processing.\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy_lefff import LefffLemmatizer, POSTagger\n",
    "import csv\n",
    "\n",
    "#nlp = spacy.load('fr')\n",
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "pos = POSTagger()\n",
    "french_lemmatizer = LefffLemmatizer(after_melt=True, default=True)\n",
    "nlp.add_pipe(pos, name='pos', after='parser')\n",
    "nlp.add_pipe(french_lemmatizer, name='lefff', after='pos')\n",
    "\n",
    "# Open our file\n",
    "with open('cleaned_text.txt', 'r') as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "    \n",
    "# Specify the information we want\n",
    "doc = nlp(text_data)\n",
    "for d in doc:\n",
    "    print(d.text, d.pos_, d._.melt_tagger, d._.lefff_lemma, d.tag_, d.lemma_)\n",
    "    \n",
    "# Output our tagged data into a CSV\n",
    "with open('text_data.csv', 'w') as csvfile:\n",
    "    fieldnames = ['text', 'pos', 'melt', 'lefff_lemma', 'tag', 'lemma']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for d in doc:\n",
    "        if d.pos_ != \"SPACE\":\n",
    "            writer.writerow({'text': d.text, 'pos': d.pos_, 'melt': d._.melt_tagger, 'lefff_lemma': d._.lefff_lemma, 'tag': d.tag_, 'lemma': d.lemma_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can perform part-of-speech tagging using the [Stanza package](https://stanfordnlp.github.io/stanza/index.html), as well. For further information about Stanza, please consult the [documentation](https://github.com/ian-nai/Non-English-NLP-Tutorial/blob/master/Documentation%20Resources.md#stanza). This [web demo](http://stanza.run/) of the package may also be of interest. \n",
    "\n",
    "#### For the purposes of this tutorial, we won't save the Stanza output to a CSV. Note that Stanza uses a different pipeline than spaCy and NLTK, and that consulting these tools' documentation is the best way to determine which one would best suit your individual needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "# Open and read our file\n",
    "with open('cleaned_text.txt', 'r') as file:\n",
    "\n",
    "    text_data = file.read()\n",
    "    \n",
    "    # Set out Stanza pipeline, including language\n",
    "    nlp = stanza.Pipeline(lang='fr', processors='tokenize, ner', tokenize_pretokenized=True)\n",
    "\n",
    "    doc = nlp(text_data)\n",
    "\n",
    "    print([token for sent in doc.sentences for token in sent.ents])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
